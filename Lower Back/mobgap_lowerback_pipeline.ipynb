{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cccabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject folder: NJQV67D\n",
      "Found 2 relevant CSV file(s) in NJQV67D: ['NJQV67D-MMMCK69HP-20210325-20210331.csv', 'NJQV67D-MMMXHXEE6-20210401-20210407.csv']\n",
      "Processing file: NJQV67D-MMMCK69HP-20210325-20210331.csv\n",
      "Starting preprocessing for file: /domino/datasets/local/dataset/idea_fast/for_s3/NJQV67D/NJQV67D-MMMCK69HP-20210325-20210331.csv\n",
      "Completed preprocessing for file: /domino/datasets/local/dataset/idea_fast/for_s3/NJQV67D/NJQV67D-MMMCK69HP-20210325-20210331.csv\n",
      "Processing file: NJQV67D-MMMXHXEE6-20210401-20210407.csv\n",
      "Starting preprocessing for file: /domino/datasets/local/dataset/idea_fast/for_s3/NJQV67D/NJQV67D-MMMXHXEE6-20210401-20210407.csv\n",
      "Completed preprocessing for file: /domino/datasets/local/dataset/idea_fast/for_s3/NJQV67D/NJQV67D-MMMXHXEE6-20210401-20210407.csv\n",
      "Combining data from 2 file(s) for subject NJQV67D\n",
      "sampling rate:  100.0\n",
      "                          cadence_spm  stride_length_m  walking_speed_mps\n",
      "gs_id sec_center_samples                                                 \n",
      "0     191352                74.534161         1.749227           1.086476\n",
      "      191452                82.191781         1.752502           1.200344\n",
      "      191552                73.619632         1.435539           0.880699\n",
      "      191652                62.500000         1.479122           0.770376\n",
      "      191752               109.090909         1.398032           1.270938\n",
      "...                               ...              ...                ...\n",
      "4291  97075030              81.632653         0.783942           0.533294\n",
      "      97075130              61.224490         0.843205           0.430207\n",
      "      97075230              68.965517         0.823100           0.473046\n",
      "      97075330              68.965517         0.792993           0.455743\n",
      "      97075430              68.965517         0.792993           0.455743\n",
      "\n",
      "[101371 rows x 3 columns]\n",
      "\n",
      "Processing subject folder: NTKCAYF\n",
      "Found 2 relevant CSV file(s) in NTKCAYF: ['NTKCAYF-MMM4APZZR-20210325-20210401.csv', 'NTKCAYF-MMMSTF39P-20210401-20210408.csv']\n",
      "Meta file not found for subject /domino/datasets/local/dataset/idea_fast/for_s3/NTKCAYF. Skipping...\n",
      "\n",
      "Processing subject folder: NTXAGNP\n",
      "No relevant CSV files found in NTXAGNP. Skipping this folder.\n",
      "\n",
      "Processing subject folder: NN4NJ39\n",
      "Found 2 relevant CSV file(s) in NN4NJ39: ['NN4NJ39-MMMQ3VRXR-20210413-20210420.csv', 'NN4NJ39-MMMVZ79SH-20210406-20210413.csv']\n",
      "Meta file not found for subject /domino/datasets/local/dataset/idea_fast/for_s3/NN4NJ39. Skipping...\n",
      "\n",
      "Processing subject folder: NY4RCQC\n",
      "Found 2 relevant CSV file(s) in NY4RCQC: ['NY4RCQC-MMM666R4P-20210203-20210209.csv', 'NY4RCQC-MMM6SJ6QQ-20210127-20210202.csv']\n",
      "Processing file: NY4RCQC-MMM666R4P-20210203-20210209.csv\n",
      "Starting preprocessing for file: /domino/datasets/local/dataset/idea_fast/for_s3/NY4RCQC/NY4RCQC-MMM666R4P-20210203-20210209.csv\n",
      "Completed preprocessing for file: /domino/datasets/local/dataset/idea_fast/for_s3/NY4RCQC/NY4RCQC-MMM666R4P-20210203-20210209.csv\n",
      "Processing file: NY4RCQC-MMM6SJ6QQ-20210127-20210202.csv\n",
      "Starting preprocessing for file: /domino/datasets/local/dataset/idea_fast/for_s3/NY4RCQC/NY4RCQC-MMM6SJ6QQ-20210127-20210202.csv\n",
      "Completed preprocessing for file: /domino/datasets/local/dataset/idea_fast/for_s3/NY4RCQC/NY4RCQC-MMM6SJ6QQ-20210127-20210202.csv\n",
      "Combining data from 2 file(s) for subject NY4RCQC\n",
      "sampling rate:  100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mobgap  # Ensure mobgap is properly installed and imported\n",
    "from mobgap.pipeline import MobilisedPipelineHealthy, GsIterator\n",
    "from mobgap.gait_sequences import GsdIluz, GsdIonescu, GsdAdaptiveIonescu\n",
    "from mobgap.utils.conversions import to_body_frame\n",
    "from mobgap.consts import GRAV_MS2\n",
    "from mobgap.initial_contacts import IcdShinImproved, refine_gs\n",
    "from mobgap.laterality import LrcUllrich\n",
    "from mobgap.stride_length import SlZijlstra\n",
    "from mobgap.turning import TdElGohary\n",
    "from mobgap.walking_speed import WsNaive\n",
    "from mobgap.cadence import CadFromIc\n",
    "\n",
    "\n",
    "\n",
    "# Define directories\n",
    "subjects_dir = '/domino/datasets/local/dataset/idea_fast/for_s3/'\n",
    "output_folder = '/mnt/Jake/final_test/'\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Preprocessing function for each file\n",
    "def preprocess_file(file_path):\n",
    "    print(f\"Starting preprocessing for file: {file_path}\")\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if 'Time' in df.columns:\n",
    "        df.rename(columns={'Time': 'timestamp'}, inplace=True)\n",
    "    # Convert timestamp column to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Standardize column names for compatibility\n",
    "    df.rename(columns={\n",
    "        'Accel-X (g)': 'acc_x',\n",
    "        'Accel-Y (g)': 'acc_y',\n",
    "        'Accel-Z (g)': 'acc_z',\n",
    "        'Gyro-X (d/s)': 'gyr_x',\n",
    "        'Gyro-Y (d/s)': 'gyr_y',\n",
    "        'Gyro-Z (d/s)': 'gyr_z',\n",
    "        'Mag-X': 'Mag_X',\n",
    "        'Mag-Y': 'Mag_Y',\n",
    "        'Mag-Z': 'Mag_Z'\n",
    "    }, inplace=True)\n",
    "    # Select relevant columns\n",
    "    df = df[['timestamp', 'acc_x', 'acc_y', 'acc_z', 'gyr_x', 'gyr_y', 'gyr_z']]\n",
    "    df[[\"acc_x\", \"acc_y\", \"acc_z\"]] = (\n",
    "        df[[\"acc_x\", \"acc_y\", \"acc_z\"]] * GRAV_MS2\n",
    "    )\n",
    "    \n",
    "    print(f\"Completed preprocessing for file: {file_path}\")\n",
    "    return df\n",
    "\n",
    "icd = IcdShinImproved()\n",
    "lrc = LrcUllrich()\n",
    "cad = CadFromIc()\n",
    "sl = SlZijlstra()\n",
    "speed = WsNaive()\n",
    "turn = TdElGohary()\n",
    "\n",
    "sampling_rate_hz = 100\n",
    "# Process each subject folder\n",
    "for subject_folder in os.listdir(subjects_dir):\n",
    "    subject_path = os.path.join(subjects_dir, subject_folder)\n",
    "    \n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(subject_path):\n",
    "        print(f\"\\nProcessing subject folder: {subject_folder}\")\n",
    "        \n",
    "        # Find relevant CSV files in the folder based on the pattern\n",
    "        csv_files = [f for f in os.listdir(subject_path) \n",
    "                     if re.match(rf\"{subject_folder}-\\w{{9}}-\\d{{8}}-\\d{{8}}\\.csv\", f)]\n",
    "        \n",
    "        # Check if any CSV files were found\n",
    "        if not csv_files:\n",
    "            print(f\"No relevant CSV files found in {subject_folder}. Skipping this folder.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(csv_files)} relevant CSV file(s) in {subject_folder}: {csv_files}\")\n",
    "        \n",
    "        # Initialize an empty list to store data from each file\n",
    "        subject_data = []\n",
    "        meta_file = os.path.join(subject_path, 'meta.csv')\n",
    "        if not os.path.exists(meta_file):\n",
    "            print(f\"Meta file not found for subject {subject_path}. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            meta_df = pd.read_csv(meta_file, header=None)\n",
    "            participant_metadata = meta_df.to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading metadata for subject {subject_folder}: {str(e)}. Skipping...\")\n",
    "            continue\n",
    "        # Process each relevant CSV file\n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(subject_path, csv_file)\n",
    "            print(f\"Processing file: {csv_file}\")\n",
    "            \n",
    "            # Preprocess the file and append the data\n",
    "            df = preprocess_file(file_path)\n",
    "            subject_data.append(df)\n",
    "        \n",
    "        # Combine data if any relevant files were found\n",
    "        if subject_data:\n",
    "            print(f\"Combining data from {len(subject_data)} file(s) for subject {subject_folder}\")\n",
    "            combined_data = pd.concat(subject_data, ignore_index=True)\n",
    "            subject_data[0]['timestamp'] = pd.to_datetime(subject_data[0]['timestamp'])\n",
    "            df_time = subject_data[0]['timestamp'] \n",
    "            time_diffs= df_time.diff().dropna()\n",
    "            avg_sampling_rate = time_diffs.mean()\n",
    "            average_sampling_rate_second = avg_sampling_rate.total_seconds()\n",
    "            sampling_rate_hz = abs(1/average_sampling_rate_second)\n",
    "            print(\"sampling rate: \", sampling_rate_hz)\n",
    "            # Sort data by timestamp in case of overlapping records\n",
    "            combined_data.sort_values(by='timestamp', inplace=True)\n",
    "            combined_data.attrs[\"participant_metadata\"] = participant_metadata\n",
    "            \n",
    "            gsd = GsdIonescu()\n",
    "            imu_data = to_body_frame(combined_data)\n",
    "            gsd.detect(data=imu_data, sampling_rate_hz=sampling_rate_hz)\n",
    "            gait_sequences = gsd.gs_list_\n",
    "            \"\"\"try:\n",
    "                start_index = gait_sequences.loc[0, 'start']\n",
    "                end_index = gait_sequences.loc[0, 'end']\n",
    "                print(combined_data.iloc[start_index:end_index,:]['mapped_value'].unique())\n",
    "            except Exception as e:\n",
    "                print(\"No gait sequences\", e)\n",
    "                continue\"\"\"\n",
    "            #print(gait_sequences)\n",
    "            gs_iterator = GsIterator()\n",
    "            for (_, gs_data), r in gs_iterator.iterate(imu_data, gait_sequences):\n",
    "                icd = icd.clone().detect(gs_data, sampling_rate_hz=sampling_rate_hz)\n",
    "                lrc = lrc.clone().predict(gs_data, icd.ic_list_, sampling_rate_hz=sampling_rate_hz)\n",
    "                r.ic_list = lrc.ic_lr_list_\n",
    "                turn = turn.clone().detect(gs_data, sampling_rate_hz=sampling_rate_hz)\n",
    "                r.turn_list = turn.turn_list_\n",
    "\n",
    "                refined_gs, refined_ic_list = refine_gs(r.ic_list)\n",
    "\n",
    "                with gs_iterator.subregion(refined_gs) as ((_, refined_gs_data), rr):\n",
    "                    cad = cad.clone().calculate(\n",
    "                        refined_gs_data,\n",
    "                        initial_contacts=refined_ic_list,\n",
    "                        sampling_rate_hz=sampling_rate_hz\n",
    "                    )\n",
    "                    rr.cadence_per_sec = cad.cadence_per_sec_\n",
    "                    sl = sl.clone().calculate(\n",
    "                        refined_gs_data,\n",
    "                        initial_contacts=refined_ic_list,\n",
    "                        sampling_rate_hz=sampling_rate_hz,\n",
    "                        sensor_height_m = 1.8\n",
    "                    )\n",
    "                    rr.stride_length_per_sec = sl.stride_length_per_sec_\n",
    "                    speed = speed.clone().calculate(\n",
    "                        refined_gs_data,\n",
    "                        initial_contacts=refined_ic_list,\n",
    "                        cadence_per_sec=cad.cadence_per_sec_,\n",
    "                        stride_length_per_sec=sl.stride_length_per_sec_,\n",
    "                        sampling_rate_hz=sampling_rate_hz\n",
    "                    )\n",
    "                    rr.walking_speed_per_sec = speed.walking_speed_per_sec_\n",
    "            results = gs_iterator.results_\n",
    "            results.ic_list\n",
    "            gait_analysis_results = pd.concat(\n",
    "                [\n",
    "                    results.cadence_per_sec,\n",
    "                    results.stride_length_per_sec,\n",
    "                    results.walking_speed_per_sec,\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            print(gait_analysis_results)\n",
    "            subject_output_dir = os.path.join(output_folder, subject_folder)\n",
    "            os.makedirs(subject_output_dir, exist_ok=True)\n",
    "            gs_list_file = os.path.join(subject_output_dir, \"gs_list.csv\")\n",
    "            gait_sequences.to_csv(gs_list_file)\n",
    "            gait_analysis_results_file = os.path.join(subject_output_dir, \"gait_analysis_results.csv\")\n",
    "            gait_analysis_results.to_csv(gait_analysis_results_file)\n",
    "    \n",
    "            #pipeline_ha = MobilisedPipelineHealthy()\n",
    "            #pipeline_ha = pipeline_ha.safe_run(combined_data)\n",
    "            #print(pipeline_ha.aggregated_parameters_) \n",
    "            \"\"\"\n",
    "            # Run gait analysis using mobgap\n",
    "            print(f\"Running gait analysis for subject {subject_folder}...\")\n",
    "            try:\n",
    "                gait_features = mobgap.process_gait(combined_data)\n",
    "                print(f\"Gait analysis completed for subject {subject_folder}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during gait analysis for subject {subject_folder}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Define output file path\n",
    "            output_file = os.path.join(output_folder, f\"{subject_folder}_gait_features.csv\")\n",
    "            \n",
    "            # Save the result\n",
    "            gait_features.to_csv(output_file, index=False)\n",
    "            print(f\"Saved gait features for {subject_folder} to {output_file}\")\n",
    "        else:\n",
    "            print(f\"No data to combine for subject {subject_folder}. Skipping gait analysis.\")\"\"\"\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
