{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdca217-581f-4208-9be3-53711a68f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a022eb1-c7e3-4b8b-b931-d89c814e4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file path to folders with gait sequence and characteristics predictions from lower back signal\n",
    "gait_dir = \"../Jake/results_mobilise_ionescu_full_redo/\"\n",
    "# Store file path to folders with signal from wrist sensor\n",
    "wrist_dir = \"dir\"\n",
    "# Output directory\n",
    "output_dir = 'Outputs/Lower Back Predictions Mapped To Wrist/'\n",
    "\n",
    "# Extract list of subjects with lower back predictions\n",
    "subjects = os.listdir(gait_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe5bc7-d6cc-412e-98d1-0add814c5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all subjects\n",
    "for folder_name in subjects:\n",
    "    print(f\"Processing subject: {folder_name}\")\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load wrist signal data\n",
    "    wrist_folder_path = os.path.join(wrist_dir, folder_name)\n",
    "    csv_path = os.path.join(wrist_folder_path, 'combined_ax6_df.csv')\n",
    "    df = pd.read_csv(csv_path, usecols=['accel_x', 'accel_y', 'accel_z'])\n",
    "    \n",
    "    signal_load_time = time.time() - start_time\n",
    "    print(f\"Wrist file load time: {signal_load_time:.2f} seconds\")\n",
    "    \n",
    "    df.reset_index(inplace=True)  # Add row numbers as an \"index\" column\n",
    "\n",
    "    # Start timer for walking_df creation\n",
    "    walking_df_start_time = time.time()\n",
    "\n",
    "    # Load gait sequence data\n",
    "    gs_folder_path = os.path.join(gait_dir, folder_name, 'gs_list.csv')\n",
    "    gs = pd.read_csv(gs_folder_path)\n",
    "\n",
    "    # Expand gait sequence to a DataFrame of indices for walking intervals\n",
    "    walking_df = pd.DataFrame({'index': np.concatenate([np.arange(row['start'], row['end'] + 1) for _, row in gs.iterrows()])})\n",
    "    walking_df['lower_back_mapped_value'] = 1\n",
    "    \n",
    "    \n",
    "    # Check for duplicates in walking_df\n",
    "    if walking_df.duplicated(subset=['index']).any():\n",
    "        print(f\"Duplicate indices found in walking_df for subject: {folder_name}\")\n",
    "        walking_df.drop_duplicates(subset=['index'], keep='first', inplace=True)\n",
    "\n",
    "    # Load gait analysis data\n",
    "    analysis_folder_path = os.path.join(gait_dir, folder_name, 'gait_analysis_results.csv')\n",
    "    gait_analysis = pd.read_csv(analysis_folder_path, usecols=['sec_center_samples', 'cadence_spm',\n",
    "                                                               'stride_length_m', 'walking_speed_mps'])\n",
    "\n",
    "    # Expand gait analysis intervals and add features\n",
    "    gait_analysis['start'] = (gait_analysis['sec_center_samples'] - 50).astype(int)\n",
    "    gait_analysis['end'] = (gait_analysis['sec_center_samples'] + 49).astype(int)\n",
    "    \n",
    "    analysis_intervals = []\n",
    "    for row in gait_analysis.itertuples(index=False):\n",
    "        for idx in range(row.start, row.end + 1):\n",
    "            analysis_intervals.append({\n",
    "                'index': idx,\n",
    "                'cadence_spm': row.cadence_spm,\n",
    "                'stride_length_m': row.stride_length_m,\n",
    "                'walking_speed_mps': row.walking_speed_mps\n",
    "            })\n",
    "    analysis_intervals = pd.DataFrame(analysis_intervals)\n",
    "    \n",
    "    # Check for duplicates in analysis_intervals\n",
    "    if analysis_intervals.duplicated(subset=['index']).any():\n",
    "        print(f\"Duplicate indices found in analysis_intervals for subject: {folder_name}\")\n",
    "        analysis_intervals.drop_duplicates(subset=['index'], keep='first', inplace=True)\n",
    "\n",
    "    # Merge the walking intervals and analysis features\n",
    "    walking_df = walking_df.merge(analysis_intervals, on='index', how='left')\n",
    "\n",
    "    walking_df_time = time.time() - walking_df_start_time\n",
    "    print(f\"Time to create walking_df: {walking_df_time:.2f} seconds\")\n",
    "\n",
    "    # Step 2: Inner join walking data to wrist signal data\n",
    "    df = df.merge(walking_df, on='index', how='inner')\n",
    "\n",
    "    # Subset and save the result\n",
    "    df = df[['index', 'accel_x', 'accel_y', 'accel_z', 'lower_back_mapped_value',\n",
    "             'cadence_spm', 'stride_length_m', 'walking_speed_mps']]\n",
    "\n",
    "    output_folder = os.path.join(output_dir, folder_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_path = os.path.join(output_folder, 'wrist_lower_back_df.csv')\n",
    "    df.to_csv(output_path, index=False, chunksize=10**6)\n",
    "\n",
    "    # End timer\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Finished processing subject: {folder_name} in {total_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
