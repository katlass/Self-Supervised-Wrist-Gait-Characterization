{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc21194-374d-48ca-af95-c071a94ae47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import joblib\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix, precision_score, recall_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"Oxford/\")\n",
    "# Import functions from data.py\n",
    "from data import NormalDataset, resize, get_inverse_class_weights\n",
    "\n",
    "# Import function from utils.py\n",
    "from utils import EarlyStopping\n",
    "\n",
    "# Need this for the pre-trained SSL model\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea324042-365c-4def-ae2a-7d66c5a16851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "wrist_dir = \"/domino/datasets/local/dataset/idea_fast/for_s3\"\n",
    "mapped_dir = \"Outputs/Lower Back Predictions Mapped To Wrist/\"\n",
    "\n",
    "# List of participants\n",
    "# subjects = ['N7TFS3G']\n",
    "subjects = os.listdir(mapped_dir)\n",
    "subjects = [subject for subject in subjects if subject not in ['.ipynb_checkpoints']]\n",
    "\n",
    "# Initialize an empty list to store all subjects' processed data\n",
    "processed_dfs = []\n",
    "\n",
    "# Columns to load\n",
    "wrist_columns_to_load = ['accel_x', 'accel_y', 'accel_z']\n",
    "mapped_columns_to_load = ['index', 'accel_x', 'accel_y', 'accel_z', 'lower_back_mapped_value']\n",
    "\n",
    "# Maximum rows to process per participant\n",
    "max_rows_per_participant = 70_000_000\n",
    "\n",
    "# Loop through each participant\n",
    "for pid, subject in enumerate(subjects, start=1):\n",
    "    print(f\"Processing subject: {subject}\")\n",
    "    \n",
    "    # Start timer for the subject\n",
    "    subject_start_time = time.time()\n",
    "\n",
    "    # Load mapped signal data\n",
    "    mapped_signal_path = os.path.join(mapped_dir, subject, 'wrist_lower_back_df.csv')\n",
    "    mapped_pd = pd.read_csv(mapped_signal_path, usecols=mapped_columns_to_load)\n",
    "    \n",
    "    # Rename 'lower_back_mapped_value' to 'label'\n",
    "    mapped_pd.rename(columns={'lower_back_mapped_value': 'label'}, inplace=True)\n",
    "    \n",
    "    # Convert accelerometer values to float32\n",
    "    mapped_pd[['accel_x', 'accel_y', 'accel_z']] = mapped_pd[['accel_x', 'accel_y', 'accel_z']].astype('float32')\n",
    "    \n",
    "    # Add participant ID column\n",
    "    mapped_pd['pid'] = pid\n",
    "    \n",
    "    # Store mapped indices in a set for fast lookups\n",
    "    mapped_indices_set = set(mapped_pd['index'])\n",
    "    \n",
    "    # Add mapped DataFrame to the list\n",
    "    processed_dfs.append(mapped_pd)\n",
    "    \n",
    "    # Load entire wrist signal data\n",
    "    whole_signal_path = os.path.join(wrist_dir, subject, 'combined_ax6_df.csv')\n",
    "\n",
    "    chunk_size = 1_000_000  # Chunk size\n",
    "    rows_read = 0  # Track the number of rows read\n",
    "\n",
    "    for chunk_idx, chunk in enumerate(pd.read_csv(whole_signal_path, usecols=wrist_columns_to_load, chunksize=chunk_size)):\n",
    "        # Start timer for the chunk\n",
    "        chunk_start_time = time.time()\n",
    "\n",
    "        # Break the loop if max rows are reached\n",
    "        if rows_read >= max_rows_per_participant:\n",
    "            print(f\"Reached max rows ({max_rows_per_participant}) for participant {subject}.\")\n",
    "            break\n",
    "        \n",
    "        # Create the `index` column for whole signal data\n",
    "        chunk.reset_index(inplace=True)  # Adds 'index' column with global row numbers\n",
    "\n",
    "        # Drop rows where accelerometer values are NaN\n",
    "        chunk = chunk.dropna(subset=['accel_x', 'accel_y', 'accel_z']).copy()\n",
    "\n",
    "        # Filter out rows in chunk that have an index present in mapped_pd\n",
    "        chunk = chunk[~chunk['index'].isin(mapped_indices_set)]\n",
    "        \n",
    "        # Add participant ID column\n",
    "        chunk.loc[:, 'pid'] = pid\n",
    "        \n",
    "        # Convert accelerometer values to float32\n",
    "        chunk.loc[:, ['accel_x', 'accel_y', 'accel_z']] = chunk[['accel_x', 'accel_y', 'accel_z']].astype('float32')\n",
    "            \n",
    "        # Assign label 0 for non-mapped data\n",
    "        chunk.loc[:, 'label'] = 0\n",
    "        \n",
    "        # Reorder columns to match mapped_df\n",
    "        chunk = chunk[['index', 'accel_x', 'accel_y', 'accel_z', 'label', 'pid']]\n",
    "        \n",
    "        # Add chunk to the processed list\n",
    "        processed_dfs.append(chunk)\n",
    "        \n",
    "        # Update the row count\n",
    "        rows_read += len(chunk)\n",
    "\n",
    "    # End timer for the subject\n",
    "    subject_total_time = time.time() - subject_start_time\n",
    "    print(f\"Finished processing subject {subject} in {subject_total_time:.2f} seconds\")\n",
    "\n",
    "# Concatenate all processed chunks into a single DataFrame\n",
    "df_combined = pd.concat(processed_dfs, ignore_index=True)\n",
    "\n",
    "# Print combined DataFrame info\n",
    "print(f\"Combined DataFrame info:\")\n",
    "print(df_combined.info())\n",
    "print(f\"Label counts:\\n{df_combined['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c774f89-a06d-4bba-b5e8-d6a87bd8fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_df(df, window_size=3000, target_freq=30, original_freq=100):\n",
    "    \"\"\"\n",
    "    Adjusts the downsampling method to match Oxford's approach using linear interpolation.\n",
    "    The function prepares data for a pre-trained SSL model, which expects 30Hz sampling and 30s windows.\n",
    "    It also balances the training and validation sets using RandomOverSampler.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame containing accelerometer data.\n",
    "        window_size (int): Number of samples per window at the original frequency (default: 3000 for 100Hz).\n",
    "        target_freq (int): Target frequency for downsampling (default: 30Hz).\n",
    "        original_freq (int): Original frequency of the data (default: 100Hz).\n",
    "\n",
    "    Returns:\n",
    "        Train, validation, and test splits with balanced training and validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the downsampled window size\n",
    "    downsampled_window_size = int(window_size * (target_freq / original_freq))  # 3000 * (30/100) = 900\n",
    "\n",
    "    # Trim excess data that doesn't fit into full windows of original size\n",
    "    num_windows = len(df) // window_size\n",
    "    df = df.iloc[:num_windows * window_size]\n",
    "\n",
    "    # Reshape the df into windows of shape (num_windows, window_size, 3)\n",
    "    X = df[['accel_x', 'accel_y', 'accel_z']].to_numpy().reshape(num_windows, window_size, 3)\n",
    "    y = df['label'].values[:num_windows * window_size].reshape(num_windows, window_size).mean(axis=1).astype(int)\n",
    "    pid = df['pid'].values[:num_windows * window_size].reshape(num_windows, window_size)[:, 0].astype(int)\n",
    "\n",
    "    # Downsample X using linear interpolation (Oxford approach)\n",
    "    from scipy.interpolate import interp1d\n",
    "    t_original = np.linspace(0, 1, window_size)  # Original time points\n",
    "    t_target = np.linspace(0, 1, downsampled_window_size)  # Target time points\n",
    "    X_downsampled = np.zeros((num_windows, downsampled_window_size, 3))  # Preallocate array\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        for axis in range(3):  # Loop over accel_x, accel_y, accel_z\n",
    "            interp_func = interp1d(t_original, X[i, :, axis], kind=\"linear\", assume_sorted=True)\n",
    "            X_downsampled[i, :, axis] = interp_func(t_target)\n",
    "\n",
    "    # Assign participants to train, validation, and test sets (60/20/20 split)\n",
    "    unique_pids = np.unique(pid)\n",
    "    train_pids, test_pids = train_test_split(unique_pids, test_size=0.2, random_state=42)\n",
    "    train_pids, val_pids = train_test_split(train_pids, test_size=0.25, random_state=41)  # 0.25 * 80% = 20%\n",
    "\n",
    "    train_idx = np.isin(pid, train_pids)\n",
    "    val_idx = np.isin(pid, val_pids)\n",
    "    test_idx = np.isin(pid, test_pids)\n",
    "\n",
    "    x_train, y_train, pid_train = X_downsampled[train_idx], y[train_idx], pid[train_idx]\n",
    "    x_val, y_val, pid_val = X_downsampled[val_idx], y[val_idx], pid[val_idx]\n",
    "    x_test, y_test, pid_test = X_downsampled[test_idx], y[test_idx], pid[test_idx]\n",
    "\n",
    "    # Balance the training and validation sets using RandomOverSampler\n",
    "    def oversample_with_noise(X, y, pid):\n",
    "        # Flatten features for oversampling\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_flat, y)\n",
    "\n",
    "        # Expand pid to match the resampled data\n",
    "        pid_resampled = ros.fit_resample(pid.reshape(-1, 1), y)[0].ravel()\n",
    "\n",
    "        # Add small random noise to avoid duplicates\n",
    "        noise = np.random.normal(0, 0.01, X_resampled.shape)\n",
    "        X_resampled += noise\n",
    "\n",
    "        # Reshape back to the original 3D format\n",
    "        return X_resampled.reshape(-1, downsampled_window_size, 3), y_resampled, pid_resampled\n",
    "\n",
    "    x_train, y_train, pid_train = oversample_with_noise(x_train, y_train, pid_train)\n",
    "    x_val, y_val, pid_val = oversample_with_noise(x_val, y_val, pid_val)\n",
    "\n",
    "    return (\n",
    "        x_train, y_train, pid_train,\n",
    "        x_val, y_val, pid_val,\n",
    "        x_test, y_test, pid_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0a69d-8e7c-4703-ac7f-3fbb0775997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function above to get training, val, and test splits\n",
    "(\n",
    "    x_train, y_train, group_train,\n",
    "    x_val, y_val, group_val,\n",
    "    x_test, y_test, group_test\n",
    ") = load_data_from_df(\n",
    "    df_combined,\n",
    "    window_size=3000,   # Original window size at 100Hz\n",
    "    target_freq=30,     # Target frequency for the SSL model\n",
    "    original_freq=100   # Original frequency of the input data\n",
    ")\n",
    "\n",
    "# Count occurrences of each label in validation and test sets\n",
    "val_classes, val_counts = np.unique(y_val, return_counts=True)\n",
    "test_classes, test_counts = np.unique(y_test, return_counts=True)\n",
    "\n",
    "print(\"Validation class distribution:\")\n",
    "for cls, count in zip(val_classes, val_counts):\n",
    "    print(f\"  Class {cls}: {count} instances\")\n",
    "\n",
    "print(\"\\nTest class distribution:\")\n",
    "for cls, count in zip(test_classes, test_counts):\n",
    "    print(f\"  Class {cls}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e7205-6df8-4809-8945-b291b26de00b",
   "metadata": {},
   "source": [
    "Run chunk below to skip fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f343dcf-ab07-4e03-88c3-f45e222f5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify fine tuning approach used\n",
    "fine_tuning_approach = \"no fine tuning\"\n",
    "\n",
    "# Load the pretrained model\n",
    "# GitHub token was free and can be replaced as needed\n",
    "os.environ['GITHUB_TOKEN'] = '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'\n",
    "\n",
    "repo = 'OxWearables/ssl-wearables'\n",
    "\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=2, pretrained=True, weights_only=False)\n",
    "sslnet = sslnet.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124b188-81bd-4dfd-856c-0c50d23f8667",
   "metadata": {},
   "source": [
    "Only run chunk below to perform fine tuning by freezing the convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32a925-8c29-45e3-8411-42ee17d72ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify fine tuning approach used\n",
    "fine_tuning_approach = \"freeze conv layers\"\n",
    "\n",
    "# Load the pretrained model\n",
    "# GitHub token used below was free and can be replaced as needed\n",
    "os.environ['GITHUB_TOKEN'] = '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'\n",
    "\n",
    "repo = 'OxWearables/ssl-wearables'\n",
    "\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=2, pretrained=True, weights_only=False)\n",
    "sslnet = sslnet.to(device).float()\n",
    "\n",
    "# Freeze the convolutional layers while keeping linear layers trainable\n",
    "def set_bn_eval(m):\n",
    "    if isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "\n",
    "# Initialize a counter to track frozen weights\n",
    "i = 0\n",
    "for name, param in sslnet.named_parameters():\n",
    "    # Check if the parameter belongs to convolutional layers\n",
    "    if \"conv\" in name or \"bn\" in name or \"feature_extractor\" in name:\n",
    "        param.requires_grad = False\n",
    "        i += 1\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Apply the batch normalization setting\n",
    "sslnet.apply(set_bn_eval)\n",
    "\n",
    "# Print the number of weights frozen in convolutional layers\n",
    "print(f\"Weights being frozen in the convolutional layers: {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9385f-7f80-4cef-a2e2-ffab1d88f4cb",
   "metadata": {},
   "source": [
    "Only run chunk below to perform fine tuning by freezing the first residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752cc914-d76e-41a7-b8ee-6fb2ad7eeccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify fine tuning approach used\n",
    "fine_tuning_approach = \"freeze first residual block\"\n",
    "\n",
    "# Load the pretrained model\n",
    "# GitHub token below was free and can be replaced as needed\n",
    "os.environ['GITHUB_TOKEN'] = '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'\n",
    "\n",
    "repo = 'OxWearables/ssl-wearables'\n",
    "\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=2, pretrained=True, weights_only=False)\n",
    "sslnet = sslnet.to(device).float()\n",
    "\n",
    "# Freeze the first residual block\n",
    "def set_bn_eval(m):\n",
    "    if isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "\n",
    "# Initialize a counter to track frozen weights\n",
    "i = 0\n",
    "for name, param in sslnet.named_parameters():\n",
    "    # Check if the parameter belongs to the first residual block\n",
    "    if name.startswith(\"feature_extractor.layer1\"):\n",
    "        param.requires_grad = False\n",
    "        i += 1\n",
    "\n",
    "# Apply the batch normalization setting\n",
    "sslnet.apply(set_bn_eval)\n",
    "\n",
    "# Print the number of weights frozen in the first residual block\n",
    "print(f\"Weights being frozen in the first residual block: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb068e59-6eeb-4ff5-8aa2-2c44d4c34d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify fine tuning approach used\n",
    "fine_tuning_approach = \"adapter layers\"\n",
    "\n",
    "# Define the adapter model\n",
    "class AdapterModel(nn.Module):\n",
    "    def __init__(self, base_model, feature_dim=1024):  # Set feature_dim based on feature extractor output\n",
    "        super(AdapterModel, self).__init__()\n",
    "        self.feature_extractor = base_model.feature_extractor\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 2)  # Output layer for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)  # Pass through the feature extractor\n",
    "        x = x.squeeze(-1)  # Remove the singleton dimension\n",
    "        x = self.adapter(x)  # Pass through the adapter layers\n",
    "        return x\n",
    "\n",
    "# Load the base pretrained model\n",
    "# GitHub token below was free and can be replaced as needed\n",
    "os.environ['GITHUB_TOKEN'] = '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'\n",
    "repo = 'OxWearables/ssl-wearables'\n",
    "\n",
    "base_model = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=2, pretrained=True, weights_only=False)\n",
    "base_model = base_model.to(device).float()\n",
    "\n",
    "# Wrap the base model with the adapter layers\n",
    "model = AdapterModel(base_model, feature_dim=1024).to(device)\n",
    "\n",
    "# Print the model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf260c7-b4ca-4954-a6ea-de32e4be3262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct datasets\n",
    "train_dataset = NormalDataset(x_train, y_train, group_train, name=\"training\", transform=True)\n",
    "val_dataset = NormalDataset(x_val, y_val, group_val, name=\"validation\")\n",
    "test_dataset = NormalDataset(x_test, y_test, group_test, name=\"test\")\n",
    "\n",
    "# Construct dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013cd496-1431-4410-9073-dbf4bcbafc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function that prioritizes precision\n",
    "class PrecisionLoss(nn.Module):\n",
    "    def __init__(self, weighted_fp=2, weighted_fn=1):\n",
    "        super().__init__()\n",
    "        self.weighted_fp = weighted_fp\n",
    "        self.weighted_fn = weighted_fn\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        pos_mask = labels.float()  # Use float precision for labels\n",
    "        fp_loss = -torch.log(probs[:, 0] + 1e-6) * (1 - pos_mask) * self.weighted_fp\n",
    "        fn_loss = -torch.log(probs[:, 1] + 1e-6) * pos_mask * self.weighted_fn\n",
    "        return fp_loss.mean() + fn_loss.mean()\n",
    "\n",
    "# Initialize with higher penalty for false positives\n",
    "loss_fn = PrecisionLoss(weighted_fp=10, weighted_fn=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bdf09-a9b0-4f35-ad57-2c8c9d55911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_precision(model, train_loader, val_loader, device, fine_tuning_approach, timestamp):\n",
    "    \"\"\"\n",
    "    Function to train a model while focusing on improving precision\n",
    "\n",
    "    Parameters:\n",
    "    - model: The neural network to be trained.\n",
    "    - train_loader: DataLoader object for training data.\n",
    "    - val_loader: DataLoader object for validation data.\n",
    "    - device: Device to run the model on ('cpu' or 'cuda').\n",
    "    - fine tuning method\n",
    "    - timestamp for outputs\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the optimizer: Adam optimizer with a learning rate of 0.0001\n",
    "    if fine_tuning_approach == \"adapter layers\":\n",
    "        # For adapter layer\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, amsgrad=True)\n",
    "        \n",
    "\n",
    "    # Set the number of epochs\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Variable to track the best validation precision seen so far\n",
    "    best_val_precision = 0\n",
    "\n",
    "    # Counter to track epochs without improvement for early stopping\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Number of epochs to wait before stopping if no improvement\n",
    "    patience = 10\n",
    "    \n",
    "\n",
    "    # Loop through each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        train_losses = []  # Keeps track of loss during training\n",
    "        \n",
    "        # Initialize accumulators for true positives and false positives\n",
    "        train_true_positives = 0\n",
    "        train_false_positives = 0\n",
    "        train_total_positives = 0\n",
    "\n",
    "        # Training Loop: Process one batch at a time from the training DataLoader\n",
    "        for batch in train_loader:\n",
    "            # Unpack the batch: inputs (features), labels (targets), optional metadata\n",
    "            if len(batch) == 3:\n",
    "                inputs, labels, _ = batch  # Extract inputs and labels\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device)\n",
    "\n",
    "            # Reset gradients to avoid accumulation from previous steps\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: Compute model predictions\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss using the custom loss function (PrecisionLoss)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass: Compute gradients for all parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model parameters based on the computed gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the training loss for analysis\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Convert model outputs to predicted labels (argmax gives class index)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "             # Record true positives and false positives directly\n",
    "            train_true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "            train_false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "            train_total_positives += (predicted == 1).sum().item()\n",
    "\n",
    "        # Compute precision for training data\n",
    "        train_precision = (\n",
    "            train_true_positives / train_total_positives\n",
    "            if train_total_positives > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        # Validation Loop: Evaluate the model on validation data\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout, batch norm updates)\n",
    "\n",
    "        # Initialize accumulators for validation precision\n",
    "        val_true_positives = 0\n",
    "        val_false_positives = 0\n",
    "        val_total_positives = 0 \n",
    "\n",
    "        # Disable gradient computation for validation (faster and saves memory)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Unpack the batch: inputs (features), labels (targets), optional metadata\n",
    "                if len(batch) == 3:\n",
    "                    inputs, labels, _ = batch\n",
    "                else:\n",
    "                    inputs, labels = batch\n",
    "\n",
    "                # Move inputs and labels to the specified device\n",
    "                inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device)\n",
    "\n",
    "                # Forward pass: Compute model predictions\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Convert model outputs to predicted labels\n",
    "                # Record true positives and false positives directly\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "                val_false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "                val_total_positives += (predicted == 1).sum().item()\n",
    "\n",
    "        # Compute precision for validation data\n",
    "        val_precision = (\n",
    "            val_true_positives / val_total_positives\n",
    "            if val_total_positives > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        \n",
    "        # Early stopping: Check if validation precision has improved\n",
    "        if val_precision > best_val_precision:\n",
    "            best_val_precision = val_precision  # Update the best precision\n",
    "            epochs_without_improvement = 0  # Reset the counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1  # Increment counter if no improvement\n",
    "\n",
    "        # Print metrics for the current epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {sum(train_losses) / len(train_losses):.4f}\")  # Average loss\n",
    "        print(f\"  Train Precision: {train_precision:.4f}\")\n",
    "        print(f\"  Validation Precision: {val_precision:.4f}\")\n",
    "        \n",
    "        # If no improvement for 'patience' epochs, stop training early\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping on epoch {epoch + 1} as validation precision did not improve for {patience} epochs.\")\n",
    "            \n",
    "            # Save the final model weights with a fine-tuning approach and timestamp\n",
    "            weights_path = os.path.join(f\"Outputs/SSL Weights Saved/model_{fine_tuning_approach}_{timestamp}.pt\")\n",
    "            torch.save(model.state_dict(), weights_path)\n",
    "            print(f\"Weights saved for epoch {epoch + 1} as {weights_path}.\")\n",
    "            \n",
    "            break\n",
    "\n",
    "def predict(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Iterate over the dataloader and do inference with a PyTorch model.\n",
    "\n",
    "    :param nn.Module model: PyTorch Module\n",
    "    :param data_loader: PyTorch DataLoader\n",
    "    :param str device: PyTorch map device ('cpu' or 'cuda')\n",
    "    :return: true labels, model predictions, pids\n",
    "    :rtype: (np.ndarray, np.ndarray, np.ndarray)\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    predictions_list = []\n",
    "    true_list = []\n",
    "    pid_list = []\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    for i, (x, y, pid) in enumerate(tqdm(data_loader)):\n",
    "        with torch.inference_mode():\n",
    "            # Ensure input tensor matches model's precision\n",
    "            x = x.to(device, dtype=torch.float)\n",
    "            logits = model(x)\n",
    "            # Collect true labels\n",
    "            true_list.append(y)\n",
    "            # Get predicted class indices\n",
    "            pred_y = torch.argmax(logits, dim=1)\n",
    "            # Append predictions and participant IDs\n",
    "            predictions_list.append(pred_y.cpu())\n",
    "            pid_list.extend(pid)\n",
    "\n",
    "    # Combine results into numpy arrays\n",
    "    true_list = torch.cat(true_list)\n",
    "    predictions_list = torch.cat(predictions_list)\n",
    "\n",
    "    return (\n",
    "        torch.flatten(true_list).numpy(),\n",
    "        torch.flatten(predictions_list).numpy(),\n",
    "        np.array(pid_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e3826-b4ae-4b98-a55c-311d4edb7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp for saving weights\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "if fine_tuning_approach == \"adapter layers\":\n",
    "    # For adapter layer\n",
    "    train_with_precision(model, train_loader, val_loader, device, fine_tuning_approach, timestamp)\n",
    "else:\n",
    "    train_with_precision(sslnet, train_loader, val_loader, device, fine_tuning_approach, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3711dd-5992-4bd2-a0e9-d88e1d23281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights from early stopping\n",
    "\n",
    "# Construct the path to the saved weights\n",
    "weights_path = os.path.join(f\"Outputs/SSL Weights Saved/model_{fine_tuning_approach}_{timestamp}.pt\")\n",
    "\n",
    "weights_path = os.path.join(f\"Outputs/SSL Weights Saved/model_adapter layers_20241201_194445.pt\")\n",
    "fine_tuning_approach = \"no fine tuning\"\n",
    "\n",
    "\n",
    "if fine_tuning_approach == \"adapter layers\":\n",
    "    # For adapter layer\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "else:\n",
    "    sslnet.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d9647-0ce8-4cf4-b201-a354db0db095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "\n",
    "if fine_tuning_approach == \"adapter layers\":\n",
    "    # For adapter layer\n",
    "    true_labels, predicted_labels, pids = predict(model, test_loader, device)\n",
    "else:\n",
    "    true_labels, predicted_labels, pids = predict(sslnet, test_loader, device)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "# Calculate overall F1-score\n",
    "overall_f1 = f1_score(true_labels, predicted_labels, average='weighted')  # 'macro', 'micro', or 'weighted'\n",
    "print(f\"\\nOverall F1 Score: {overall_f1:.2f}\")\n",
    "\n",
    "# Calculate overall precision\n",
    "overall_precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "print(f\"\\nOverall Precision: {overall_precision:.2f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
