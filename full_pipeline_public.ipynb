{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5489e2-531d-4bc2-b624-54473e9e7860",
   "metadata": {},
   "source": [
    "# Spatio-temporal Gait Characterization from a Wrist-Worn Inertial Measurement Units in Daily Life\n",
    "\n",
    "## Objective\n",
    "The primary goal of this project is to develop and validate algorithms that can accurately characterize spatio-temporal gait parameters using data from a wrist-worn IMU. \n",
    "\n",
    "Specific objectives include: \n",
    "* Apply open-source lower back gait detection algorithms to partially labeled lower back data to validate the best approach for generating labels for the whole dataset.\n",
    "* Use generated labels from the lower back model to train a custom DCNN to make gait detection predictions on unsupervised wrist-worn sensor data.\n",
    "* Based on this self-supervised learning approach, use the data from predicted walking bouts to characterize subjectsâ€™ gait focusing on walking speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687edd2d-ad4d-4707-b0ea-0e829eeea9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Mobgap Imports\n",
    "import mobgap  # Ensure mobgap is properly installed and imported\n",
    "from mobgap.pipeline import MobilisedPipelineHealthy, GsIterator\n",
    "from mobgap.gait_sequences import GsdIluz, GsdIonescu, GsdAdaptiveIonescu\n",
    "from mobgap.utils.conversions import to_body_frame\n",
    "from mobgap.consts import GRAV_MS2\n",
    "from mobgap.initial_contacts import IcdShinImproved, refine_gs\n",
    "from mobgap.laterality import LrcUllrich\n",
    "from mobgap.stride_length import SlZijlstra\n",
    "from mobgap.turning import TdElGohary\n",
    "from mobgap.walking_speed import WsNaive\n",
    "from mobgap.cadence import CadFromIc\n",
    "\n",
    "# Custom Imports\n",
    "from data import NormalDataset, resize, get_inverse_class_weights\n",
    "from utils import EarlyStopping\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479502b-f6da-4bac-bfc5-8bd3967f0a75",
   "metadata": {},
   "source": [
    "## Lumbar Activity Classification and Gait Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7829ac-f212-40ae-8aa9-543d1efd250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "subjects_dir = '/XXXX/'\n",
    "output_folder = '/XXXX/'\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Preprocessing function for each file\n",
    "def preprocess_file(file_path):\n",
    "    print(f\"Starting preprocessing for file: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if 'Time' in df.columns:\n",
    "        df.rename(columns={'Time': 'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    # Standardize column names for compatibility\n",
    "    df.rename(columns={\n",
    "        'Accel-X (g)': 'acc_x',\n",
    "        'Accel-Y (g)': 'acc_y',\n",
    "        'Accel-Z (g)': 'acc_z',\n",
    "        'Gyro-X (d/s)': 'gyr_x',\n",
    "        'Gyro-Y (d/s)': 'gyr_y',\n",
    "        'Gyro-Z (d/s)': 'gyr_z',\n",
    "        'Mag-X': 'Mag_X',\n",
    "        'Mag-Y': 'Mag_Y',\n",
    "        'Mag-Z': 'Mag_Z'\n",
    "    }, inplace=True)\n",
    "    # Select relevant columns\n",
    "    df = df[['timestamp', 'acc_x', 'acc_y', 'acc_z', 'gyr_x', 'gyr_y', 'gyr_z']]\n",
    "    df[[\"acc_x\", \"acc_y\", \"acc_z\"]] = (\n",
    "        df[[\"acc_x\", \"acc_y\", \"acc_z\"]] * GRAV_MS2\n",
    "    )\n",
    "    print(f\"Completed preprocessing for file: {file_path}\")\n",
    "    return df\n",
    "\n",
    "icd = IcdShinImproved()\n",
    "lrc = LrcUllrich()\n",
    "cad = CadFromIc()\n",
    "sl = SlZijlstra()\n",
    "speed = WsNaive()\n",
    "turn = TdElGohary()\n",
    "\n",
    "sampling_rate_hz = 100\n",
    "# Process each subject folder\n",
    "for subject_folder in os.listdir(subjects_dir):\n",
    "    subject_path = os.path.join(subjects_dir, subject_folder)\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(subject_path):\n",
    "        print(f\"\\nProcessing subject folder: {subject_folder}\")\n",
    "        \n",
    "        # Find relevant CSV files in the folder based on the pattern\n",
    "        csv_files = [f for f in os.listdir(subject_path) \n",
    "                     if re.match(rf\"{subject_folder}-\\w{{9}}-\\d{{8}}-\\d{{8}}\\.csv\", f)]\n",
    "        \n",
    "        # Check if any CSV files were found\n",
    "        if not csv_files:\n",
    "            print(f\"No relevant CSV files found in {subject_folder}. Skipping this folder.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(csv_files)} relevant CSV file(s) in {subject_folder}: {csv_files}\")\n",
    "        \n",
    "        # Initialize an empty list to store data from each file\n",
    "        subject_data = []\n",
    "        meta_file = os.path.join(subject_path, 'meta.csv')\n",
    "        if not os.path.exists(meta_file):\n",
    "            print(f\"Meta file not found for subject {subject_path}. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            meta_df = pd.read_csv(meta_file, header=None)\n",
    "            participant_metadata = meta_df.to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading metadata for subject {subject_folder}: {str(e)}. Skipping...\")\n",
    "            continue\n",
    "        # Process each relevant CSV file\n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(subject_path, csv_file)\n",
    "            print(f\"Processing file: {csv_file}\")\n",
    "            \n",
    "            # Preprocess the file and append the data\n",
    "            df = preprocess_file(file_path)\n",
    "            subject_data.append(df)\n",
    "        \n",
    "        # Combine data if any relevant files were found\n",
    "        if subject_data:\n",
    "            print(f\"Combining data from {len(subject_data)} file(s) for subject {subject_folder}\")\n",
    "            combined_data = pd.concat(subject_data, ignore_index=True)\n",
    "            subject_data[0]['timestamp'] = pd.to_datetime(subject_data[0]['timestamp'])\n",
    "            df_time = subject_data[0]['timestamp'] \n",
    "            time_diffs= df_time.diff().dropna()\n",
    "            avg_sampling_rate = time_diffs.mean()\n",
    "            average_sampling_rate_second = avg_sampling_rate.total_seconds()\n",
    "            sampling_rate_hz = abs(1/average_sampling_rate_second)\n",
    "            print(\"sampling rate: \", sampling_rate_hz)\n",
    "            # Sort data by timestamp in case of overlapping records\n",
    "            combined_data.sort_values(by='timestamp', inplace=True)\n",
    "            combined_data.attrs[\"participant_metadata\"] = participant_metadata\n",
    "            \n",
    "            gsd = GsdIonescu()\n",
    "            imu_data = to_body_frame(combined_data)\n",
    "            gsd.detect(data=imu_data, sampling_rate_hz=sampling_rate_hz)\n",
    "            gait_sequences = gsd.gs_list_\n",
    "            \"\"\"try:\n",
    "                start_index = gait_sequences.loc[0, 'start']\n",
    "                end_index = gait_sequences.loc[0, 'end']\n",
    "                print(combined_data.iloc[start_index:end_index,:]['mapped_value'].unique())\n",
    "            except Exception as e:\n",
    "                print(\"No gait sequences\", e)\n",
    "                continue\"\"\"\n",
    "            gs_iterator = GsIterator()\n",
    "            for (_, gs_data), r in gs_iterator.iterate(imu_data, gait_sequences):\n",
    "                icd = icd.clone().detect(gs_data, sampling_rate_hz=sampling_rate_hz)\n",
    "                lrc = lrc.clone().predict(gs_data, icd.ic_list_, sampling_rate_hz=sampling_rate_hz)\n",
    "                r.ic_list = lrc.ic_lr_list_\n",
    "                turn = turn.clone().detect(gs_data, sampling_rate_hz=sampling_rate_hz)\n",
    "                r.turn_list = turn.turn_list_\n",
    "\n",
    "                refined_gs, refined_ic_list = refine_gs(r.ic_list)\n",
    "\n",
    "                with gs_iterator.subregion(refined_gs) as ((_, refined_gs_data), rr):\n",
    "                    cad = cad.clone().calculate(\n",
    "                        refined_gs_data,\n",
    "                        initial_contacts=refined_ic_list,\n",
    "                        sampling_rate_hz=sampling_rate_hz\n",
    "                    )\n",
    "                    rr.cadence_per_sec = cad.cadence_per_sec_\n",
    "                    sl = sl.clone().calculate(\n",
    "                        refined_gs_data,\n",
    "                        initial_contacts=refined_ic_list,\n",
    "                        sampling_rate_hz=sampling_rate_hz,\n",
    "                        sensor_height_m = 1.8\n",
    "                    )\n",
    "                    rr.stride_length_per_sec = sl.stride_length_per_sec_\n",
    "                    speed = speed.clone().calculate(\n",
    "                        refined_gs_data,\n",
    "                        initial_contacts=refined_ic_list,\n",
    "                        cadence_per_sec=cad.cadence_per_sec_,\n",
    "                        stride_length_per_sec=sl.stride_length_per_sec_,\n",
    "                        sampling_rate_hz=sampling_rate_hz\n",
    "                    )\n",
    "                    rr.walking_speed_per_sec = speed.walking_speed_per_sec_\n",
    "            results = gs_iterator.results_\n",
    "            results.ic_list\n",
    "            gait_analysis_results = pd.concat(\n",
    "                [\n",
    "                    results.cadence_per_sec,\n",
    "                    results.stride_length_per_sec,\n",
    "                    results.walking_speed_per_sec,\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            print(gait_analysis_results)\n",
    "            subject_output_dir = os.path.join(output_folder, subject_folder)\n",
    "            os.makedirs(subject_output_dir, exist_ok=True)\n",
    "            gs_list_file = os.path.join(subject_output_dir, \"gs_list.csv\")\n",
    "            gait_sequences.to_csv(gs_list_file)\n",
    "            gait_analysis_results_file = os.path.join(subject_output_dir, \"gait_analysis_results.csv\")\n",
    "            gait_analysis_results.to_csv(gait_analysis_results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb79f29-d06f-4ce3-8edc-da4fb86e151a",
   "metadata": {},
   "source": [
    "## Self-Superision: Lumbar Predictons are Labels for Wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c440c-40f9-4dad-b052-4c5f2a16e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file path to folders with gait sequence predictions from lower back signal\n",
    "gait_dir = \"/XXXX/\"\n",
    "# Store file path to folders with signal from wrist sensor\n",
    "wrist_dir = \"/XXXX/\"\n",
    "# Output directory\n",
    "output_dir = '/XXXX/'\n",
    "\n",
    "# Extract list of subjects with lower back predictions\n",
    "subjects = os.listdir(gait_dir)\n",
    "# Exclude two subjects with no demographics data\n",
    "subjects = [subject for subject in subjects if subject not in ['XXXX', 'XXXX']]\n",
    "\n",
    "\n",
    "# Loop through all subjects\n",
    "for folder_name in subjects:\n",
    "    print(f\"Processing subject: {folder_name}\")\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    # Load wrist signal data\n",
    "    wrist_folder_path = os.path.join(wrist_dir, folder_name)\n",
    "    csv_path = os.path.join(wrist_folder_path, 'wrist_data.csv')\n",
    "    df = pd.read_csv(csv_path, usecols=['accel_x', 'accel_y', 'accel_z'])\n",
    "    signal_load_time = time.time() - start_time\n",
    "    print(f\"Wrist file load time: {signal_load_time:.2f} seconds\")\n",
    "    df.reset_index(inplace=True)  # Add row numbers as an \"index\" column\n",
    "\n",
    "    # Start timer for walking_df creation\n",
    "    walking_df_start_time = time.time()\n",
    "    # Load gait sequence data\n",
    "    gs_folder_path = os.path.join(gait_dir, folder_name, 'gs_list.csv')\n",
    "    gs = pd.read_csv(gs_folder_path)\n",
    "    # Expand gait sequence to a DataFrame of indices for walking intervals\n",
    "    walking_df = pd.DataFrame({'index': np.concatenate([np.arange(row['start'], row['end'] + 1) for _, row in gs.iterrows()])})\n",
    "    walking_df['lower_back_mapped_value'] = 1\n",
    "    # Check for duplicates in walking_df\n",
    "    if walking_df.duplicated(subset=['index']).any():\n",
    "        print(f\"Duplicate indices found in walking_df for subject: {folder_name}\")\n",
    "        walking_df.drop_duplicates(subset=['index'], keep='first', inplace=True)\n",
    "    # Load gait analysis data\n",
    "    analysis_folder_path = os.path.join(gait_dir, folder_name, 'gait_analysis_results.csv')\n",
    "    gait_analysis = pd.read_csv(analysis_folder_path, usecols=['sec_center_samples', 'cadence_spm',\n",
    "                                                               'stride_length_m', 'walking_speed_mps'])\n",
    "    # Expand gait analysis intervals and add features\n",
    "    gait_analysis['start'] = (gait_analysis['sec_center_samples'] - 50).astype(int)\n",
    "    gait_analysis['end'] = (gait_analysis['sec_center_samples'] + 49).astype(int)\n",
    "    \n",
    "    analysis_intervals = []\n",
    "    for row in gait_analysis.itertuples(index=False):\n",
    "        for idx in range(row.start, row.end + 1):\n",
    "            analysis_intervals.append({\n",
    "                'index': idx,\n",
    "                'cadence_spm': row.cadence_spm,\n",
    "                'stride_length_m': row.stride_length_m,\n",
    "                'walking_speed_mps': row.walking_speed_mps\n",
    "            })\n",
    "    analysis_intervals = pd.DataFrame(analysis_intervals)\n",
    "    # Check for duplicates in analysis_intervals\n",
    "    if analysis_intervals.duplicated(subset=['index']).any():\n",
    "        print(f\"Duplicate indices found in analysis_intervals for subject: {folder_name}\")\n",
    "        analysis_intervals.drop_duplicates(subset=['index'], keep='first', inplace=True)\n",
    "    # Merge the walking intervals and analysis features\n",
    "    walking_df = walking_df.merge(analysis_intervals, on='index', how='left')\n",
    "    walking_df_time = time.time() - walking_df_start_time\n",
    "    print(f\"Time to create walking_df: {walking_df_time:.2f} seconds\")\n",
    "    \n",
    "    # Step 2: Inner join walking data to wrist signal data\n",
    "    df = df.merge(walking_df, on='index', how='inner')\n",
    "    df = df[['index', 'accel_x', 'accel_y', 'accel_z', 'lower_back_mapped_value',\n",
    "             'cadence_spm', 'stride_length_m', 'walking_speed_mps']]\n",
    "\n",
    "    output_folder = os.path.join(output_dir, folder_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_path = os.path.join(output_folder, 'wrist_lower_back_df.csv')\n",
    "    df.to_csv(output_path, index=False, chunksize=10**6)\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Finished processing subject: {folder_name} in {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f9988-1ddf-4bbd-aec1-b0bd709d7d50",
   "metadata": {},
   "source": [
    "\n",
    "## Wrist Activity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfa88a-6296-4687-98e6-6ac3325cc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"Oxford/\")\n",
    "device = 'cpu'\n",
    "wrist_dir = '/XXXX/'\n",
    "mapped_dir = '/XXXX/'\n",
    "\n",
    "# List of participants\n",
    "subjects = os.listdir(mapped_dir)\n",
    "subjects = [subject for subject in subjects if subject not in ['.ipynb_checkpoints']]\n",
    "\n",
    "# Initialize an empty list to store all subjects' processed data, select columns\n",
    "processed_dfs = []\n",
    "wrist_columns_to_load = ['accel_x', 'accel_y', 'accel_z']\n",
    "mapped_columns_to_load = ['index', 'accel_x', 'accel_y', 'accel_z', 'lower_back_mapped_value']\n",
    "# Maximum rows to process per participant\n",
    "max_rows_per_participant = 70_000_000\n",
    "\n",
    "# Loop through each participant\n",
    "for pid, subject in enumerate(subjects, start=1):\n",
    "    print(f\"Processing subject: {subject}\")\n",
    "    subject_start_time = time.time()\n",
    "    # Load mapped signal data\n",
    "    mapped_signal_path = os.path.join(mapped_dir, subject, 'wrist_lower_back_df.csv')\n",
    "    mapped_pd = pd.read_csv(mapped_signal_path, usecols=mapped_columns_to_load)\n",
    "    # Rename 'lower_back_mapped_value' to 'label'\n",
    "    mapped_pd.rename(columns={'lower_back_mapped_value': 'label'}, inplace=True)\n",
    "    # Convert accelerometer values to float32\n",
    "    mapped_pd[['accel_x', 'accel_y', 'accel_z']] = mapped_pd[['accel_x', 'accel_y', 'accel_z']].astype('float32')\n",
    "    # Add participant ID column\n",
    "    mapped_pd['pid'] = pid\n",
    "    # Store mapped indices in a set for fast lookups\n",
    "    mapped_indices_set = set(mapped_pd['index'])\n",
    "    # Add mapped DataFrame to the list\n",
    "    processed_dfs.append(mapped_pd)\n",
    "   \n",
    "    # Load entire wrist signal data\n",
    "    whole_signal_path = os.path.join(wrist_dir, subject, 'wrist_data.csv')\n",
    "    chunk_size = 1_000_000  # Chunk size\n",
    "    rows_read = 0  # Track the number of rows read\n",
    "    for chunk_idx, chunk in enumerate(pd.read_csv(whole_signal_path, usecols=wrist_columns_to_load, chunksize=chunk_size)):\n",
    "        chunk_start_time = time.time()\n",
    "        # Break the loop if max rows are reached\n",
    "        if rows_read >= max_rows_per_participant:\n",
    "            print(f\"Reached max rows ({max_rows_per_participant}) for participant {subject}.\")\n",
    "            break\n",
    "        # Create the `index` column for whole signal data\n",
    "        chunk.reset_index(inplace=True)  # Adds 'index' column with global row numbers\n",
    "        # Drop rows where accelerometer values are NaN\n",
    "        chunk = chunk.dropna(subset=['accel_x', 'accel_y', 'accel_z']).copy()\n",
    "        # Filter out rows in chunk that have an index present in mapped_pd\n",
    "        chunk = chunk[~chunk['index'].isin(mapped_indices_set)]\n",
    "        # Add participant ID column\n",
    "        chunk.loc[:, 'pid'] = pid\n",
    "        # Convert accelerometer values to float32\n",
    "        chunk.loc[:, ['accel_x', 'accel_y', 'accel_z']] = chunk[['accel_x', 'accel_y', 'accel_z']].astype('float32')\n",
    "        # Assign label 0 for non-mapped data\n",
    "        chunk.loc[:, 'label'] = 0\n",
    "        # Reorder columns to match mapped_df\n",
    "        chunk = chunk[['index', 'accel_x', 'accel_y', 'accel_z', 'label', 'pid']]\n",
    "        # Add chunk to the processed list\n",
    "        processed_dfs.append(chunk)\n",
    "        # Update the row count\n",
    "        rows_read += len(chunk)\n",
    "    subject_total_time = time.time() - subject_start_time\n",
    "    print(f\"Finished processing subject {subject} in {subject_total_time:.2f} seconds\")\n",
    "\n",
    "# Concatenate all processed chunks into a single DataFrame\n",
    "df_combined = pd.concat(processed_dfs, ignore_index=True)\n",
    "print(f\"Combined DataFrame info:\")\n",
    "print(df_combined.info())\n",
    "print(f\"Label counts:\\n{df_combined['label'].value_counts()}\")\n",
    "\n",
    "\n",
    "def load_data_from_df(df, window_size=3000, target_freq=30, original_freq=100):\n",
    "    \"\"\"\n",
    "    Adjusts the downsampling method to match Oxford's approach using linear interpolation.\n",
    "    The function prepares data for a pre-trained SSL model, which expects 30Hz sampling and 30s windows.\n",
    "    It also balances the training and validation sets using RandomOverSampler.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame containing accelerometer data.\n",
    "        window_size (int): Number of samples per window at the original frequency (default: 3000 for 100Hz).\n",
    "        target_freq (int): Target frequency for downsampling (default: 30Hz).\n",
    "        original_freq (int): Original frequency of the data (default: 100Hz).\n",
    "\n",
    "    Returns:\n",
    "        Train, validation, and test splits with balanced training and validation sets.\n",
    "    \"\"\"\n",
    "    # Calculate the downsampled window size\n",
    "    downsampled_window_size = int(window_size * (target_freq / original_freq))  # 3000 * (30/100) = 900\n",
    "    # Trim excess data that doesn't fit into full windows of original size\n",
    "    num_windows = len(df) // window_size\n",
    "    df = df.iloc[:num_windows * window_size]\n",
    "    # Reshape the df into windows of shape (num_windows, window_size, 3)\n",
    "    X = df[['accel_x', 'accel_y', 'accel_z']].to_numpy().reshape(num_windows, window_size, 3)\n",
    "    y = df['label'].values[:num_windows * window_size].reshape(num_windows, window_size).mean(axis=1).astype(int)\n",
    "    pid = df['pid'].values[:num_windows * window_size].reshape(num_windows, window_size)[:, 0].astype(int)\n",
    "\n",
    "    # Downsample X using linear interpolation (Oxford approach)\n",
    "    t_original = np.linspace(0, 1, window_size)  # Original time points\n",
    "    t_target = np.linspace(0, 1, downsampled_window_size)  # Target time points\n",
    "    X_downsampled = np.zeros((num_windows, downsampled_window_size, 3))  # Preallocate array\n",
    "    for i in range(num_windows):\n",
    "        for axis in range(3):  # Loop over accel_x, accel_y, accel_z\n",
    "            interp_func = interp1d(t_original, X[i, :, axis], kind=\"linear\", assume_sorted=True)\n",
    "            X_downsampled[i, :, axis] = interp_func(t_target)\n",
    "\n",
    "    # Assign participants to train, validation, and test sets (60/20/20 split)\n",
    "    unique_pids = np.unique(pid)\n",
    "    train_pids, test_pids = train_test_split(unique_pids, test_size=0.2, random_state=42)\n",
    "    train_pids, val_pids = train_test_split(train_pids, test_size=0.25, random_state=41)  # 0.25 * 80% = 20%\n",
    "    train_idx = np.isin(pid, train_pids)\n",
    "    val_idx = np.isin(pid, val_pids)\n",
    "    test_idx = np.isin(pid, test_pids)\n",
    "    x_train, y_train, pid_train = X_downsampled[train_idx], y[train_idx], pid[train_idx]\n",
    "    x_val, y_val, pid_val = X_downsampled[val_idx], y[val_idx], pid[val_idx]\n",
    "    x_test, y_test, pid_test = X_downsampled[test_idx], y[test_idx], pid[test_idx]\n",
    "\n",
    "    # Balance the training and validation sets using RandomOverSampler\n",
    "    def oversample_with_noise(X, y, pid):\n",
    "        # Flatten features for oversampling\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_flat, y)\n",
    "        # Expand pid to match the resampled data\n",
    "        pid_resampled = ros.fit_resample(pid.reshape(-1, 1), y)[0].ravel()\n",
    "        # Add small random noise to avoid duplicates\n",
    "        noise = np.random.normal(0, 0.01, X_resampled.shape)\n",
    "        X_resampled += noise\n",
    "        return X_resampled.reshape(-1, downsampled_window_size, 3), y_resampled, pid_resampled\n",
    "\n",
    "    x_train, y_train, pid_train = oversample_with_noise(x_train, y_train, pid_train)\n",
    "    x_val, y_val, pid_val = oversample_with_noise(x_val, y_val, pid_val)\n",
    "\n",
    "    return (\n",
    "        x_train, y_train, pid_train,\n",
    "        x_val, y_val, pid_val,\n",
    "        x_test, y_test, pid_test\n",
    "    )\n",
    "\n",
    "# Execution\n",
    "(\n",
    "    x_train, y_train, group_train,\n",
    "    x_val, y_val, group_val,\n",
    "    x_test, y_test, group_test\n",
    ") = load_data_from_df(\n",
    "    df_combined,\n",
    "    window_size=3000,   # Original window size at 100Hz\n",
    "    target_freq=30,     # Target frequency for the SSL model\n",
    "    original_freq=100   # Original frequency of the input data\n",
    ")\n",
    "\n",
    "# Count occurrences of each label in validation and test sets\n",
    "val_classes, val_counts = np.unique(y_val, return_counts=True)\n",
    "test_classes, test_counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Validation class distribution:\")\n",
    "for cls, count in zip(val_classes, val_counts):\n",
    "    print(f\"  Class {cls}: {count} instances\")\n",
    "print(\"\\nTest class distribution:\")\n",
    "for cls, count in zip(test_classes, test_counts):\n",
    "    print(f\"  Class {cls}: {count} instances\")\n",
    "\n",
    "\n",
    "# Load the pretrained model\n",
    "os.environ['GITHUB_TOKEN'] = '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'\n",
    "repo = 'OxWearables/ssl-wearables'\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=2, pretrained=True, weights_only=False)\n",
    "sslnet = sslnet.to(device).float()\n",
    "# Specify fine tuning approach used\n",
    "fine_tuning_approach = \"no fine tuning\"\n",
    "\n",
    "# Approach 1: Freeze the convolutional layers while keeping linear layers trainable\n",
    "fine_tuning_approach = \"freeze conv layers\"\n",
    "os.environ['GITHUB_TOKEN'] = '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'\n",
    "repo = 'OxWearables/ssl-wearables'\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=2, pretrained=True, weights_only=False)\n",
    "sslnet = sslnet.to(device).float()\n",
    "def set_bn_eval(m):\n",
    "    if isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "i = 0\n",
    "for name, param in sslnet.named_parameters():\n",
    "    # Check if the parameter belongs to convolutional layers (typically in feature extractor)\n",
    "    if \"conv\" in name or \"bn\" in name or \"feature_extractor\" in name:\n",
    "        param.requires_grad = False  # Freeze convolutional layers\n",
    "        i += 1\n",
    "    else:\n",
    "        param.requires_grad = True  # Keep linear layers trainable\n",
    "\n",
    "# Apply the batch normalization setting\n",
    "sslnet.apply(set_bn_eval)\n",
    "print(f\"Weights being frozen in the convolutional layers: {i}\")\n",
    "\n",
    "# Approach 2:Freeze the first residual block\n",
    "fine_tuning_approach = \"freeze first residual block\"\n",
    "os.environ['GITHUB_TOKEN'] = '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'\n",
    "repo = 'OxWearables/ssl-wearables'\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=2, pretrained=True, weights_only=False)\n",
    "sslnet = sslnet.to(device).float()\n",
    "def set_bn_eval(m):\n",
    "    if isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "i = 0\n",
    "for name, param in sslnet.named_parameters():\n",
    "    # Check if the parameter belongs to the first residual block\n",
    "    if name.startswith(\"feature_extractor.layer1\"):\n",
    "        param.requires_grad = False\n",
    "        i += 1\n",
    "# Apply the batch normalization setting\n",
    "sslnet.apply(set_bn_eval)\n",
    "print(f\"Weights being frozen in the first residual block: {i}\")\n",
    "\n",
    "# Approach 3: Adapter Layers\n",
    "class AdapterModel(nn.Module):\n",
    "    def __init__(self, base_model, feature_dim=1024):  # Set feature_dim based on feature extractor output\n",
    "        super(AdapterModel, self).__init__()\n",
    "        self.feature_extractor = base_model.feature_extractor\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 2)  # Output layer for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x) \n",
    "        x = x.squeeze(-1)  \n",
    "        x = self.adapter(x)  \n",
    "        return x\n",
    "\n",
    "fine_tuning_approach = \"adapter layers\"\n",
    "os.environ['GITHUB_TOKEN'] = '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'\n",
    "repo = 'OxWearables/ssl-wearables'\n",
    "base_model = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=2, pretrained=True, weights_only=False)\n",
    "base_model = base_model.to(device).float()\n",
    "# Wrap the base model with the adapter layers\n",
    "model = AdapterModel(base_model, feature_dim=1024).to(device)\n",
    "print(model)\n",
    "\n",
    "# Construct datasets\n",
    "train_dataset = NormalDataset(x_train, y_train, group_train, name=\"training\", transform=True)\n",
    "val_dataset = NormalDataset(x_val, y_val, group_val, name=\"validation\")\n",
    "test_dataset = NormalDataset(x_test, y_test, group_test, name=\"test\")\n",
    "\n",
    "# Construct dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Custom Precision Loss Function\n",
    "class PrecisionLoss(nn.Module):\n",
    "    def __init__(self, weighted_fp=2, weighted_fn=1):\n",
    "        super().__init__()\n",
    "        self.weighted_fp = weighted_fp\n",
    "        self.weighted_fn = weighted_fn\n",
    "    def forward(self, outputs, labels):\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        pos_mask = labels.float()  # Use float precision for labels\n",
    "        fp_loss = -torch.log(probs[:, 0] + 1e-6) * (1 - pos_mask) * self.weighted_fp\n",
    "        fn_loss = -torch.log(probs[:, 1] + 1e-6) * pos_mask * self.weighted_fn\n",
    "        return fp_loss.mean() + fn_loss.mean()\n",
    "# Initialize with higher penalty for false positives\n",
    "loss_fn = PrecisionLoss(weighted_fp=10, weighted_fn=1).to(device)\n",
    "\n",
    "\n",
    "def train_with_precision(model, train_loader, val_loader, device, fine_tuning_approach, timestamp):\n",
    "    if fine_tuning_approach == \"adapter layers\":\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, amsgrad=True)\n",
    "    num_epochs = 20\n",
    "    best_val_precision = 0 # Variable to track the best validation precision seen so far\n",
    "    epochs_without_improvement = 0\n",
    "    patience = 10     # Number of epochs to wait before stopping if no improvement\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        train_losses = []  # Keeps track of loss during training\n",
    "        # Initialize accumulators for true positives and false positives\n",
    "        train_true_positives = 0\n",
    "        train_false_positives = 0\n",
    "        train_total_positives = 0\n",
    "        \n",
    "        # Training Loop\n",
    "        for batch in train_loader:\n",
    "            # Unpack the batch: inputs (features), labels (targets), optional metadata\n",
    "            if len(batch) == 3:\n",
    "                inputs, labels, _ = batch  # Extract inputs and labels\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device)\n",
    "            # Reset gradients to avoid accumulation from previous steps\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: Compute model predictions\n",
    "            outputs = model(inputs)\n",
    "            # Compute the loss using the custom loss function (PrecisionLoss)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # Backward pass: Compute gradients for all parameters\n",
    "            loss.backward()\n",
    "            # Update the model parameters based on the computed gradients\n",
    "            optimizer.step()\n",
    "            # Record the training loss for analysis\n",
    "            train_losses.append(loss.item())\n",
    "            # Convert model outputs to predicted labels (argmax gives class index)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "             # Record true positives and false positives directly\n",
    "            train_true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "            train_false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "            train_total_positives += (predicted == 1).sum().item()\n",
    "\n",
    "        # Compute precision for training data\n",
    "        train_precision = (\n",
    "            train_true_positives / train_total_positives\n",
    "            if train_total_positives > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        # Validation Loop:\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout, batch norm updates)\n",
    "        # Initialize accumulators for validation precision\n",
    "        val_true_positives = 0\n",
    "        val_false_positives = 0\n",
    "        val_total_positives = 0 \n",
    "        # Disable gradient computation for validation (faster and saves memory)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Unpack the batch: inputs (features), labels (targets), optional metadata\n",
    "                if len(batch) == 3:\n",
    "                    inputs, labels, _ = batch\n",
    "                else:\n",
    "                    inputs, labels = batch\n",
    "                # Move inputs and labels to the specified device\n",
    "                inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device)\n",
    "                # Forward pass: Compute model predictions\n",
    "                outputs = model(inputs)\n",
    "                # Convert model outputs to predicted labels\n",
    "                # Record true positives and false positives directly\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "                val_false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "                val_total_positives += (predicted == 1).sum().item()\n",
    "        # Compute precision for validation data\n",
    "        val_precision = (\n",
    "            val_true_positives / val_total_positives\n",
    "            if val_total_positives > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_precision > best_val_precision:\n",
    "            best_val_precision = val_precision  # Update the best precision\n",
    "            epochs_without_improvement = 0  # Reset the counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1  # Increment counter if no improvement\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {sum(train_losses) / len(train_losses):.4f}\")  # Average loss\n",
    "        print(f\"  Train Precision: {train_precision:.4f}\")\n",
    "        print(f\"  Validation Precision: {val_precision:.4f}\")\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping on epoch {epoch + 1} as validation precision did not improve for {patience} epochs.\")\n",
    "            weights_path = os.path.join(f\"Outputs/SSL Weights Saved/model_{fine_tuning_approach}_{timestamp}.pt\")\n",
    "            torch.save(model.state_dict(), weights_path)\n",
    "            print(f\"Weights saved for epoch {epoch + 1} as {weights_path}.\")\n",
    "            break\n",
    "\n",
    "def predict(model, data_loader, device):\n",
    "    predictions_list = []\n",
    "    true_list = []\n",
    "    pid_list = []\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    for i, (x, y, pid) in enumerate(tqdm(data_loader)):\n",
    "        with torch.inference_mode():\n",
    "            # Ensure input tensor matches model's precision\n",
    "            x = x.to(device, dtype=torch.float)\n",
    "            logits = model(x)\n",
    "            true_list.append(y)\n",
    "            pred_y = torch.argmax(logits, dim=1)\n",
    "            predictions_list.append(pred_y.cpu())\n",
    "            pid_list.extend(pid)\n",
    "\n",
    "    # Combine results into numpy arrays\n",
    "    true_list = torch.cat(true_list)\n",
    "    predictions_list = torch.cat(predictions_list)\n",
    "    return (\n",
    "        torch.flatten(true_list).numpy(),\n",
    "        torch.flatten(predictions_list).numpy(),\n",
    "        np.array(pid_list),\n",
    "    )\n",
    "\n",
    "# Get the current timestamp for saving weights\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "if fine_tuning_approach == \"adapter layers\":\n",
    "    train_with_precision(model, train_loader, val_loader, device, fine_tuning_approach, timestamp)\n",
    "else:\n",
    "    train_with_precision(sslnet, train_loader, val_loader, device, fine_tuning_approach, timestamp)\n",
    "\n",
    "# Load the best model weights from early stopping\n",
    "weights_path = os.path.join(f\"XXXX.pt\")\n",
    "\n",
    "fine_tuning_approach = \"no fine tuning\"\n",
    "if fine_tuning_approach == \"adapter layers\":\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "else:\n",
    "    sslnet.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "missing_keys, unexpected_keys = sslnet.load_state_dict(torch.load(weights_path), strict=False)\n",
    "print(f\"Missing keys: {missing_keys}\")\n",
    "print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "if fine_tuning_approach == \"adapter layers\":\n",
    "    true_labels, predicted_labels, pids = predict(model, test_loader, device)\n",
    "else:\n",
    "    true_labels, predicted_labels, pids = predict(sslnet, test_loader, device)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "test_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "overall_f1 = f1_score(true_labels, predicted_labels, average='weighted')  # 'macro', 'micro', or 'weighted'\n",
    "print(f\"\\nOverall F1 Score: {overall_f1:.2f}\")\n",
    "overall_precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "print(f\"\\nOverall Precision: {overall_precision:.2f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "# Plot confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c06e7-1ec6-4049-9fdd-59d293201e91",
   "metadata": {},
   "source": [
    "## Wrist Gait Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aafc6ccd-ea80-473d-a999-4e07a344615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Absolute Error (MAE): 0.0224\n",
      "Average RÂ² Score: 0.9627\n",
      "Average Adjusted RÂ² Score: 0.9627\n",
      "Average Mean Squared Error (MSE): 0.0013\n",
      "Average Root Mean Squared Error (RMSE): 0.0349\n"
     ]
    }
   ],
   "source": [
    "def wrist_gait_speed():\n",
    "    gait_dir = \"/XXXX/\" # Store file path to folders with gait sequence predictions from lower back signal\n",
    "    wrist_dir = \"/XXXX/\" # Store file path to folders with signal from wrist sensor\n",
    "    output_dir = '/XXXX/' # Output directory\n",
    "    \n",
    "    # Extract list of subjects with lower back predictions\n",
    "    subjects = os.listdir(gait_dir)\n",
    "    # Exclude two subjects with no demographics data\n",
    "    subjects = [subject for subject in subjects if subject not in ['XXXXX', 'XXXXX']]\n",
    "\n",
    "    # Capture accuracy metrics\n",
    "    maes=[]\n",
    "    r_s=[]\n",
    "    adj_rs=[]\n",
    "    mses=[]\n",
    "    rmses=[]\n",
    "    # Train regression model only on the first subject, validate with all other subjects\n",
    "    train_first=True \n",
    "    # Loop through all subjects\n",
    "    for folder_name in subjects:\n",
    "        print(f\"Processing subject: {folder_name}\")\n",
    "        start_time = time.time()\n",
    "        # Load wrist signal data\n",
    "        wrist_folder_path = os.path.join(wrist_dir, folder_name)\n",
    "        csv_path = os.path.join(wrist_folder_path, 'wrist_data.csv')\n",
    "        df = pd.read_csv(csv_path, usecols=['accel_x', 'accel_y', 'accel_z'])\n",
    "        signal_load_time = time.time() - start_time\n",
    "        print(f\"Wrist file load time: {signal_load_time:.2f} seconds\")\n",
    "        df.reset_index(inplace=True)  # Add row numbers as an \"index\" column\n",
    "    \n",
    "        walking_df_start_time = time.time()\n",
    "        # Load lumbar gait sequence data\n",
    "        gs_folder_path = os.path.join(gait_dir, folder_name, 'gs_list.csv')\n",
    "        gs = pd.read_csv(gs_folder_path)\n",
    "        # Expand gait sequence to a DataFrame of indices for walking intervals\n",
    "        walking_df = pd.DataFrame({'index': np.concatenate([np.arange(row['start'], row['end'] + 1) for _, row in gs.iterrows()])})\n",
    "        walking_df['lower_back_mapped_value'] = 1\n",
    "        # Check for duplicates in walking_df\n",
    "        if walking_df.duplicated(subset=['index']).any():\n",
    "            print(f\"Duplicate indices found in walking_df for subject: {folder_name}\")\n",
    "            walking_df.drop_duplicates(subset=['index'], keep='first', inplace=True)\n",
    "        \n",
    "        # Load gait analysis data\n",
    "        analysis_folder_path = os.path.join(gait_dir, folder_name, 'gait_analysis_results.csv')\n",
    "        gait_analysis = pd.read_csv(analysis_folder_path, usecols=['sec_center_samples', 'cadence_spm',\n",
    "                                                                   'stride_length_m', 'walking_speed_mps'])\n",
    "        # Expand gait analysis intervals and add features\n",
    "        gait_analysis['start'] = (gait_analysis['sec_center_samples'] - 50).astype(int)\n",
    "        gait_analysis['end'] = (gait_analysis['sec_center_samples'] + 49).astype(int)\n",
    "        analysis_intervals = []\n",
    "        for row in gait_analysis.itertuples(index=False):\n",
    "            for idx in range(row.start, row.end + 1):\n",
    "                analysis_intervals.append({\n",
    "                    'index': idx,\n",
    "                    'cadence_spm': row.cadence_spm,\n",
    "                    'stride_length_m': row.stride_length_m,\n",
    "                    'walking_speed_mps': row.walking_speed_mps\n",
    "                })\n",
    "        analysis_intervals = pd.DataFrame(analysis_intervals)\n",
    "        # Check for duplicates in analysis_intervals\n",
    "        if analysis_intervals.duplicated(subset=['index']).any():\n",
    "            print(f\"Duplicate indices found in analysis_intervals for subject: {folder_name}\")\n",
    "            analysis_intervals.drop_duplicates(subset=['index'], keep='first', inplace=True)\n",
    "        # Merge the walking intervals and analysis features\n",
    "        walking_df = walking_df.merge(analysis_intervals, on='index', how='left')\n",
    "        walking_df_time = time.time() - walking_df_start_time\n",
    "        print(f\"Time to create walking_df: {walking_df_time:.2f} seconds\")\n",
    "    \n",
    "        # Step 2: Inner join walking data to wrist signal data\n",
    "        df = df.merge(walking_df, on='index', how='inner')\n",
    "        df=df[pd.isna(df.walking_speed_mps)==False]\n",
    "\n",
    "        # Fitting the OLS regression\n",
    "        if train_first ==True: # Only train regression on first patient \n",
    "            X_test=df[[ 'accel_x', 'accel_y', 'accel_z', 'cadence_spm', 'stride_length_m']][0:50000] # Test set is the last 50,000 observations of the first patient's data\n",
    "            X=df[[ 'accel_x', 'accel_y', 'accel_z', 'cadence_spm', 'stride_length_m']][50000:] # Train is the rest of the first patient's data\n",
    "            y_test=df['walking_speed_mps'][0:50000]\n",
    "            y=df['walking_speed_mps'][50000:]\n",
    "            X = sm.add_constant(X)  \n",
    "            model = sm.OLS(y, X).fit()\n",
    "            print(model.summary())\n",
    "            train_first=False\n",
    "        else:\n",
    "            X_test=df[[ 'accel_x', 'accel_y', 'accel_z', 'cadence_spm', 'stride_length_m']] # Use all other patients to validate OLS results in terms of speed prediction \n",
    "            y_test=df['walking_speed_mps']\n",
    "        X_test = sm.add_constant(X_test)\n",
    "        predictions = model.predict(X_test)\n",
    "        try:\n",
    "            # Get evaluation metrics\n",
    "            mae=mean_absolute_error(y_test, predictions)\n",
    "            mse=mean_squared_error(y_test, predictions)\n",
    "            rmse=np.sqrt(mse)\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            n = len(X_test)  \n",
    "            p = 5\n",
    "            adjusted_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n",
    "            print(\"Predictions vs Truth:\")\n",
    "            comp = pd.DataFrame({\"Truth\": y_test, \"Prediction\": predictions})\n",
    "            print(comp)\n",
    "            print(\"\\nScoring Metrics:\")\n",
    "            print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "            print(f\"Mean Squared Error (MSE):{mse:.4f}\")\n",
    "            \n",
    "            print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "            print(f\"RÂ² Score: {r2:.4f}\")\n",
    "            print(f\"Adjusted RÂ² Score:{adjusted_r2:.4f}\")\n",
    "            maes=maes+[float(mae)]\n",
    "            r_s=r_s+ [float(r2)]\n",
    "            adj_rs=adj_rs+[float(adjusted_r2)]\n",
    "            mses=mses+ [float(mse)]\n",
    "            rmses=rmses+[float(rmse)]\n",
    "            results=[maes,r_s,adj_rs,mses,rmses]\n",
    "            # Saving intermediate results in-case failure\n",
    "            # with open(\"/XXXXX.pkl\", \"wb\") as file:  # \"wb\" = write binary mode\n",
    "            #     pickle.dump(results, file)\n",
    "            # print(\"Data has been saved successfully!\")\n",
    "        except: #If it fails, add patient's ID to the list for further examination and continue on to next patient\n",
    "            maes=maes+[folder_name]\n",
    "            r_s=r_s+ [folder_name]\n",
    "            adj_rs=adj_rs+[folder_name]\n",
    "            mses=mses+ [folder_name]\n",
    "            rmses=rmses+[folder_name]\n",
    "            results=[maes,r_s,adj_rs,mses,rmses]\n",
    "            # Saving intermediate results in-case failure\n",
    "            # with open(\"/XXXX.pkl\", \"wb\") as file:  # \"wb\" = write binary mode\n",
    "            #     pickle.dump(results, file)\n",
    "            # print(\"Data has been saved successfully, skipped file: \",folder_name)\n",
    "    return model, results\n",
    "\n",
    "# Function to average accuracy metrics\n",
    "def avg_metrics(lst):\n",
    "    numeric_values = [x for x in lst if isinstance(x, (int, float))]\n",
    "    return sum(numeric_values) / len(numeric_values) \n",
    "    \n",
    "\n",
    "OLS_model, OLS_metrics=wrist_gait_speed()\n",
    "\n",
    "#Averaging results\n",
    "avg_scores=[]\n",
    "titles=[\"Average Mean Absolute Error (MAE):\",\"Average RÂ² Score:\",\"Average Adjusted RÂ² Score:\",\"Average Mean Squared Error (MSE):\",\"Average Root Mean Squared Error (RMSE):\"]\n",
    "for x in OLS_metrics:\n",
    "    avg_scores=avg_scores+[avg_metrics(x)]\n",
    "for x in range(len(avg_scores)):\n",
    "    print(titles[x],round(avg_scores[x],4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
